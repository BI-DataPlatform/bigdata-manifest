# Dockerfile for Hadoop worker 

# Download ubuntu base image 20.04
FROM ubuntu:20.04
LABEL Dongpil Yu <dongpilYu95@gmail.com>

WORKDIR /root

# bash를 사용하도록 설정
SHELL ["/bin/bash", "-c"]

ENV HADOOP_VERSION=3.2.2
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_HOME=/usr/local/hadoop/hadoop-${HADOOP_VERSION} 
ENV PATH=$PATH:$HADOOP_HOME/bin
ENV HADOOP_SLAVE_NUMBER 2
ENV HADOOP_UID=1001
ENV HADOOP_GID=1001

# hadoop 사용자와 그룹 생성
RUN groupadd -r hadoop -g ${HADOOP_GID} && \
    useradd -r -s /bin/bash -g hadoop -m hadoop -u ${HADOOP_UID} && \
    mkdir -p /home/hadoop && \
    mkdir -p /home/hadoop/.ssh

# 필수 패키지 설치
RUN apt-get clean && \
    apt-get update && \
    apt-get install --only-upgrade openssl libssl1.1 libexpat1 && \
    apt-get install -y libk5crypto3 libkrb5-3 libsqlite3-0 && \
    apt-get install -y openssh-server openjdk-8-jdk wget nano iputils-ping net-tools telnet

# hadoop 사용자 ssh key 생성 및 권한 부여
# RUN ssh-keygen -t rsa -f /home/hadoop/.ssh/id_rsa -P '' && \
#     cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys && \
#     chown -R hadoop:hadoop /home/hadoop && \
#     chmod 700 /home/hadoop/.ssh && \
#     chmod 600 /home/hadoop/.ssh/authorized_keys

# ssh without key
# RUN ssh-keygen -t rsa -f /root/.ssh/id_rsa -P '' && \
#     cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys

# Install HADOOP ${HADOOP_VERSION}
# RUN wget https://archive.apache.org/dist/hadoop/core/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
#     tar -xzvf hadoop-${HADOOP_VERSION}.tar.gz && \
#     mv hadoop-${HADOOP_VERSION} /usr/local/hadoop && \
#     rm hadoop-${HADOOP_VERSION}.tar.gz

# 로컬에 받아 둔 hadoop 압축 파일 해제
ADD hadoop-conf/hadoop-${HADOOP_VERSION}.tar.gz /usr/local/hadoop

# Create the default directories
RUN mkdir -p /home/hadoop/hdfs/datanode && \ 
    # mkdir -p /home/hadoop/hdfs/namenode && \ 
    mkdir -p /home/hadoop/hdfs/hadooptmp && \
    mkdir -p $HADOOP_HOME/logs

# Copy config files
ADD hadoop-conf/ssh_config /root/.ssh/config
ADD hadoop-conf/ssh_config /home/hadoop/.ssh/config
ADD hadoop-conf/hadoop-env.sh $HADOOP_HOME/etc/hadoop/hadoop-env.sh
ADD hadoop-conf/worker/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml
ADD hadoop-conf/worker/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml
ADD hadoop-conf/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml
ADD hadoop-conf/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml
ADD hadoop-conf/capacity-scheduler.xml ${HADOOP_HOME}/etc/hadoop/capacity-scheduler.xml
# ADD hadoop-conf/slaves $HADOOP_HOME/etc/hadoop/slaves
ADD hadoop-conf/include_list $HADOOP_HOME/etc/hadoop/include_list
ADD hadoop-conf/worker/start_hadoop.sh /home/hadoop/start_hadoop.sh
ADD hadoop-conf/run_wordcount.sh /home/hadoop/run_wordcount.sh
ADD hadoop-conf/worker/start-dfs.sh ${HADOOP_HOME}/sbin/start-dfs.sh
ADD hadoop-conf/worker/start-yarn.sh ${HADOOP_HOME}/sbin/start-yarn.sh
ADD hadoop-conf/hadoop-functions.sh ${HADOOP_HOME}/libexec/hadoop-functions.sh


# hadoop 사용자의 SSH 키 추가
COPY hadoop-conf/hadoop_rsa /home/hadoop/.ssh/id_rsa
COPY hadoop-conf/hadoop_rsa.pub /home/hadoop/.ssh/id_rsa.pub
COPY hadoop-conf/hadoop_rsa.pub /home/hadoop/.ssh/authorized_keys

# root 사용자의 SSH 키 추가 (필요한 경우)
COPY hadoop-conf/hadoop_rsa /root/.ssh/id_rsa
COPY hadoop-conf/hadoop_rsa.pub /root/.ssh/id_rsa.pub
COPY hadoop-conf/hadoop_rsa.pub /root/.ssh/authorized_keys

RUN chmod +x /home/hadoop/start_hadoop.sh && \
    chmod +x /home/hadoop/run_wordcount.sh && \
    # chmod +x $HADOOP_HOME/etc/hadoop/slaves && \
    chmod +x $HADOOP_HOME/sbin/start-dfs.sh && \
    chmod +x $HADOOP_HOME/sbin/start-yarn.sh 

# 권한 설정
RUN chown -R hadoop:hadoop /home/hadoop/.ssh && \
    chmod 700 /home/hadoop/.ssh && \
    chmod 600 /home/hadoop/.ssh/id_rsa /home/hadoop/.ssh/authorized_keys && \
    chmod 644 /home/hadoop/.ssh/id_rsa.pub && \
    chown -R root:root /root/.ssh && \
    chmod 700 /root/.ssh && \
    chmod 600 /root/.ssh/id_rsa /root/.ssh/authorized_keys && \
    chmod 644 /root/.ssh/id_rsa.pub

RUN chown -R hadoop:hadoop $HADOOP_HOME
RUN chown -R hadoop:hadoop /home/hadoop

# Format namenode
# RUN $HADOOP_HOME/bin/hdfs namenode -format
# RUN chown -R hadoop:hadoop /home/hadoop/hdfs/namenode

CMD [ "bash", "-c", "service ssh start; /home/hadoop/start_hadoop.sh;"]

# HDFS ports (master)
# EXPOSE 8020, 50070

# HDFS ports (worker)
# EXPOSE 50010 50020 50075 

# Mapred ports (common)
# EXPOSE 19888 

# Yarn ports (master)
# EXPOSE 8030 8031 8032 8033 8088

# Yarn ports (worker)
# EXPOSE 8040 8042 

# HIVE port (master)
# EXPOSE 9083 10000