# Dockerfile for Hadoop master with hive

# Download ubuntu base image 20.04
FROM ubuntu:20.04
LABEL Dongpil Yu <dongpilYu95@gmail.com>

WORKDIR /root

# bash를 사용하도록 설정
SHELL ["/bin/bash", "-c"]

ENV HADOOP_VERSION=3.2.2
ENV HADOOP_UID=1001
ENV HADOOP_GID=1001
ENV HIVE_VERSION=3.1.3
ENV LOG4J_VERSION=2.20.0
ENV LOG4J_LOCATION="https://repo1.maven.org/maven2/org/apache/logging/log4j"
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_HOME=/usr/local/hadoop/hadoop-${HADOOP_VERSION} 
ENV HIVE_HOME=/opt/local/hive/apache-hive-${HIVE_VERSION}-bin
ENV PATH=$PATH:$HADOOP_HOME/bin:$HIVE_HOME/bin 
ENV HADOOP_SLAVE_NUMBER 2

RUN mkdir -p ${HIVE_HOME}

# hadoop 사용자와 그룹 생성
RUN groupadd -r hadoop -g ${HADOOP_GID} && \
    useradd -r -s /bin/bash -g hadoop -m hadoop -u ${HADOOP_UID} && \
    mkdir -p /home/hadoop && \
    mkdir -p /home/hadoop/.ssh

# 필수 패키지 설치
RUN apt-get clean && \
    apt-get update && \
    apt-get install --only-upgrade openssl libssl1.1 libexpat1 && \
    apt-get install -y libk5crypto3 libkrb5-3 libsqlite3-0 && \
    apt-get install -y openssh-server openjdk-8-jdk wget nano iputils-ping net-tools telnet

# hadoop 사용자 ssh key 생성 및 권한 부여
# RUN ssh-keygen -t rsa -f /home/hadoop/.ssh/id_rsa -P '' && \
#    cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys && \
#    chown -R hadoop:hadoop /home/hadoop && \
#    chmod 700 /home/hadoop/.ssh && \
#    chmod 600 /home/hadoop/.ssh/authorized_keys

# ssh without key
# RUN ssh-keygen -t rsa -f /root/.ssh/id_rsa -P '' && \
#    cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys

# Install HADOOP ${HADOOP_VERSION}
# RUN wget https://archive.apache.org/dist/hadoop/core/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
#     tar -xzvf hadoop-3.2.2.tar.gz && \
#     mv hadoop-3.2.2 /usr/local/hadoop && \
#     rm hadoop-3.2.2.tar.gz

# 로컬에 받아 둔 hadoop 압축 파일 해제
ADD hadoop-${HADOOP_VERSION}.tar.gz /usr/local/hadoop

# 로컬에 받아 둔 hive 압축 파일 해제
ADD apache-hive-${HIVE_VERSION}-bin.tar.gz /opt/local/hive/


# Create the default directories
RUN mkdir -p /home/hadoop/hdfs/namenode && \ 
    mkdir -p /home/hadoop/hdfs/datanode && \
    mkdir -p /home/hadoop/hdfs/hadooptmp && \
    mkdir -p $HADOOP_HOME/logs

# Copy resources from the host to the docker container
ADD hadoop-conf/ssh_config /root/.ssh/config
ADD hadoop-conf/ssh_config /home/hadoop/.ssh/config
ADD hadoop-conf/hadoop-env.sh $HADOOP_HOME/etc/hadoop/hadoop-env.sh
ADD hadoop-conf/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml
ADD hadoop-conf/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml
ADD hadoop-conf/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml
ADD hadoop-conf/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml
ADD hadoop-conf/capacity-scheduler.xml ${HADOOP_HOME}/etc/hadoop/capacity-scheduler.xml
ADD hadoop-conf/slaves $HADOOP_HOME/etc/hadoop/slaves
ADD hadoop-conf/include_list $HADOOP_HOME/etc/hadoop/include_list
ADD hadoop-conf/start_hadoop.sh /home/hadoop/start_hadoop.sh
ADD hadoop-conf/run_wordcount.sh /home/hadoop/run_wordcount.sh
ADD hadoop-conf/start-dfs.sh ${HADOOP_HOME}/sbin/start-dfs.sh
ADD hadoop-conf/hadoop-functions.sh ${HADOOP_HOME}/libexec/hadoop-functions.sh

# hadoop 사용자의 SSH 키 추가
COPY hadoop-conf/hadoop_rsa /home/hadoop/.ssh/id_rsa
COPY hadoop-conf/hadoop_rsa.pub /home/hadoop/.ssh/id_rsa.pub
COPY hadoop-conf/hadoop_rsa.pub /home/hadoop/.ssh/authorized_keys

# root 사용자의 SSH 키 추가 (필요한 경우)
COPY hadoop-conf/hadoop_rsa /root/.ssh/id_rsa
COPY hadoop-conf/hadoop_rsa.pub /root/.ssh/id_rsa.pub
COPY hadoop-conf/hadoop_rsa.pub /root/.ssh/authorized_keys

# MySQL Connector/J 추가
# ADD https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar ${HIVE_HOME}/lib/mysql-connector-java.jar
ADD mysql-connector-java-8.0.28.jar ${HIVE_HOME}/lib/mysql-connector-java.jar

ADD hive-conf ${HIVE_HOME}/conf
ADD hive-conf/scripts/entrypoint.sh ${HIVE_HOME}/entrypoint.sh

RUN chmod +x /home/hadoop/start_hadoop.sh && \
    chmod +x /home/hadoop/run_wordcount.sh && \
    chmod +x $HADOOP_HOME/etc/hadoop/slaves && \
    chmod +x $HADOOP_HOME/sbin/start-dfs.sh && \
    chmod +x $HADOOP_HOME/sbin/start-yarn.sh && \
    chmod +x $HIVE_HOME/entrypoint.sh

# 권한 설정
RUN chown -R hadoop:hadoop /home/hadoop/.ssh && \
    chmod 700 /home/hadoop/.ssh && \
    chmod 600 /home/hadoop/.ssh/id_rsa /home/hadoop/.ssh/authorized_keys && \
    chmod 644 /home/hadoop/.ssh/id_rsa.pub && \
    chown -R root:root /root/.ssh && \
    chmod 700 /root/.ssh && \
    chmod 600 /root/.ssh/id_rsa /root/.ssh/authorized_keys && \
    chmod 644 /root/.ssh/id_rsa.pub

RUN chown -R hadoop:hadoop $HADOOP_HOME
RUN chown -R hadoop:hadoop /home/hadoop
# RUN chown -R hadoop:hadoop $HADOOP_HOME/sbin
# RUN chown -R hadoop:hadoop $HADOOP_HOME/etc/hadoop
# RUN chown hadoop:hadoop $HADOOP_HOME/logs
# RUN chown -R hadoop:hadoop /home/hadoop

RUN chown hadoop:hadoop /home/hadoop

# Format namenode
RUN $HADOOP_HOME/bin/hdfs namenode -format

RUN chown -R hadoop:hadoop /home/hadoop/hdfs/namenode

# https://docs.oracle.com/javase/7/docs/technotes/guides/net/properties.html
# Java caches dns results forever, don't cache dns results forever:
# RUN touch ${JAVA_HOME}/lib/security/java.security
# RUN sed -i '/networkaddress.cache.ttl/d' ${JAVA_HOME}/lib/security/java.security
# RUN sed -i '/networkaddress.cache.negative.ttl/d' ${JAVA_HOME}/lib/security/java.security
# RUN echo 'networkaddress.cache.ttl=0' >> ${JAVA_HOME}/lib/security/java.security
# RUN echo 'networkaddress.cache.negative.ttl=0' >> ${JAVA_HOME}/lib/security/java.security

RUN mkdir -p /var/lib/hive /root/.beeline

# Start services. NOTE: the /bin/bash has to run finally to keep the container running
CMD [ "bash", "-c", "service ssh start; /home/hadoop/start_hadoop.sh; $HIVE_HOME/entrypoint.sh"]
# CMD ["/root/start_hadoop.sh", "-d"]

# HDFS ports
# EXPOSE 50010 50020 50070 50075 50090 8020 9000
# # Mapred ports
# EXPOSE 10020 19888
# # Yarn ports
# EXPOSE 8030 8031 8032 8033 8040 8042 8088
# # HIVE port
# EXPOSE 9083